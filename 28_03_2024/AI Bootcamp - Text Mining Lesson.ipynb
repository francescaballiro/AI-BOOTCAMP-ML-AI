{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05438449-eb06-44bd-977e-1873b3761f8e",
   "metadata": {},
   "source": [
    "# About Me  \n",
    "\n",
    "My name is **Federico Barusolo**, and Iâ€™m a **Computer Engineer** with a Master of Science (MoS) path specializing in **Artificial Intelligence, Data Science, Machine Learning**.  \n",
    "\n",
    "I studied at **Politecnico di Milano** (2013-2018), worked in consultancy (2018-2021) and startup (2021 to date). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b025899-77c5-4338-86bf-d6f1eee11130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings as wn\n",
    "\n",
    "wn.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7273b51-d477-487f-b7cf-2b00d27d1f4a",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eacccf1-06bd-421e-9850-c38f6f055de5",
   "metadata": {},
   "source": [
    "# ðŸ“– Introduction to Text Mining  \n",
    "\n",
    "## What is Text Mining?  \n",
    "\n",
    "**Text Mining** is a set of techniques and methodologies used to extract meaningful information from unstructured textual data. By leveraging **natural language processing (NLP), machine learning, and statistical analysis**, text mining enables the identification of patterns, relationships, and valuable insights within text data.  \n",
    "\n",
    "## Applications  \n",
    "Text mining is widely used across various fields, including:  \n",
    "- **Automatic document classification** for organizing and managing large volumes of information\n",
    "- **Information Retrieval** to find documents matching some specific queries  \n",
    "- **Information extraction** to identify names, locations, events, and key concepts in text\n",
    "- **Sentiment analysis** to understand opinions and trends in social media and reviews\n",
    "- **Fraud detection and anomaly recognition** to uncover hidden patterns in financial or legal documents\n",
    "- **Natural Language Understanding and Generation** (arguably the hottest trends right now)\n",
    "\n",
    "Text mining plays a crucial role in transforming raw text into actionable knowledge, driving data-driven decision-making.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1272ce1-5c17-480d-8b1d-fddd67f3ae5d",
   "metadata": {},
   "source": [
    "#### Structured Data v Unstructured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d944aef-ca73-4dac-b7d7-63432ca4ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a structured dataset (CSV format)\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Carol\"],\n",
    "    \"Age\": [25, 30, 35, 40],\n",
    "    \"Salary\": [50000, 60000, 70000, 80000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the structured data\n",
    "print(\"Structured Data (Tabular Format):\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afd2e6-547a-4091-8647-a9e44c4be789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average Salary\n",
    "average_salary = \n",
    "print(f\"\\nAverage Salary: {average_salary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd480b55-c07f-4410-8262-6310d573eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the age of Charlie\n",
    "charlies_age = \n",
    "\n",
    "print(f\"\\nCharlie's age: {charlies_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22e3eb-15e4-427b-907d-809e999e5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Simple example of \"unstructured\" text\n",
    "text_data = \"\"\"\n",
    "Alice is 25 years old and earns $50,000. Bob is 30 years old and earns $60,000.\n",
    "Charlie is 35 years old and earns $70,000. Carol is 40 years old and earns $80,000.\n",
    "\"\"\"\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2b76c-4b1d-47c2-aa7d-7fbe13826403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the previous Name-Age-Salary table from the given text\n",
    "\n",
    "df = \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdf189-f87a-4c5c-b8d1-926a850f457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world unstructured text\n",
    "text_data = \"\"\"\n",
    "Alice is 25 years old and earns $50,000. She has less experience than Bob, who's 30, \n",
    "and has a salary of $60000. Then Charlie (35) earns 70k dollars annually, and Carol, who's\n",
    "forty years old, is the most senior and therefore earns $10,000 more than Charlie.\n",
    "\"\"\"\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a696f-9ecb-4b00-a4bb-67a600b5dd21",
   "metadata": {},
   "source": [
    "## Did You Know?  \n",
    "Approximately **80% of the world's data is unstructured**! Unlike structured data, which is neatly organized in tables and databases, unstructured data lacks a predefined format, making it more complex to process and analyze.\n",
    "\n",
    "## Examples of textual unstructured data:\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260e6bb-aee2-46da-b192-baf5ea1f4d68",
   "metadata": {},
   "source": [
    "##Â Document Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055e908-ae94-4e14-a1cd-c1212020d13c",
   "metadata": {},
   "source": [
    "####Â Bag-of-words (BoW)\n",
    "\n",
    "Bag of Words (BoW) is a text representation model used in NLP. It represents a document as a set of words, ignoring grammar and word order. Each document is converted into a vector based on word frequency from a predefined vocabulary. While simple and effective for many tasks, BoW does not capture word meaning or context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d139c-025a-481b-b987-330c8612f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"\n",
    "Bad is so bad, that we cannot but think good an accident; \n",
    "good is so good, that we feel certain that evil could be explained.\n",
    "\"\"\"\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5278049-7ad5-4473-b297-c252ff1d3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some basic cleaning\n",
    "clean_text = \n",
    "\n",
    "#Â create a vocabulary\n",
    "vocabulary = \n",
    "print(f\"vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cadcc-df36-4311-bc85-7897d2dee9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the text into a BoW\n",
    "bow = \n",
    "\n",
    "# sort by occurrences and then alphabetically\n",
    "print(sorted(bow, key=lambda x: (-x[1], x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7a668-2a76-4174-ac51-ef085036a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... or just use the proper tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([clean_text])\n",
    "\n",
    "bow = list(zip(vectorizer.get_feature_names_out(), X.toarray()[0]))\n",
    "result = sorted(bow, key=lambda x: (-x[1], x[0]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a520c-e5a5-47e8-800e-9821774c0c86",
   "metadata": {},
   "source": [
    "#### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807baf7a-7b3a-425b-831d-b0dc49833cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency of each term in text\n",
    "\n",
    "tf = \n",
    "\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de115f66-2018-4936-95f0-148e7e5f877d",
   "metadata": {},
   "source": [
    "####Â Term Frequency - Inverse Document Frequency\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection (or corpus). It balances the frequency of a term within a document and its rarity across the entire corpus, helping to highlight significant terms.\n",
    "\n",
    "TF-IDF = TF(t, d) * IDF(t) = TF(t, d) * log(N/DF(t))\n",
    "\n",
    "where:\n",
    "- TF(t, d): frequency of term t in document d\n",
    "- IDF(t): inverse frequency of term t across all documents\n",
    "- N: number of documents\n",
    "- DF(t): number of documents containing term t\n",
    "- the logarithm aims at dampening the effect of inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae4e4a-aabe-4e85-a73c-8ea8841cf9b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "\"\"\"\n",
    "Recipe: Carbonara; Ingredients: pasta, guanciale, eggs, pecorino romano cheese, black pepper, salt.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "The ingredients you need to use to make this recipe (Carbonara) are: pasta, eggs, roman pecorino, guanciale, salt and black pepper.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "Recipe - the Carbonara: eggs, cream, guanciale, pasta, salt, pecorino cheese, black pepper.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "For making carbonara you need these ingredients: egg, guanciale, pasta, salt, pecorino roman cheese and black pepper.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "To make roman carbonara you need to use the eggs, guanciale, pecorino, pasta, salt and pepper.\n",
    "\"\"\"]\n",
    "\n",
    "for ix, doc in enumerate(documents):\n",
    "    print(f\"Document {ix + 1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8b535-7c0e-4ac8-9d22-8c5f28e10fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â perform same cleaning as before\n",
    "clean_documents = [re.sub(r\"[^\\w\\s]\", \"\", x.replace(\"\\n\", \"\").lower()) for x in documents]\n",
    "print(clean_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31dd630-4d6e-4585-8cd7-701b67df0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(clean_documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the result to a dense array (optional, but easier to read)\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Print the TF-IDF matrix with corresponding words\n",
    "df = pd.DataFrame(dense_matrix, index=[f\"doc{x+1}\" for x in range(len(clean_documents))], columns=feature_names)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70e3ae-1c6c-4f7f-900c-a8cccd93b516",
   "metadata": {},
   "source": [
    "## Further pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6891a4-1005-44da-9a7b-76ed029537d3",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is a text normalization technique in **Natural Language Processing (NLP)** that reduces words to their root or base form. It helps in **reducing inflected or derived words** to a common base, which improves text analysis tasks such as search and classification.\n",
    "\n",
    "For example:\n",
    "- **\"running\" â†’ \"run\"**\n",
    "- **\"flies\" â†’ \"fli\"** (stemming may not always produce a valid word)\n",
    "- **\"better\" â†’ \"better\"** (stemming does not handle lemmatization)\n",
    "\n",
    "One of the most commonly used stemming algorithms is **Porterâ€™s Stemmer**, which applies a set of rules to remove common suffixes from words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0eee3-48c2-4097-a8d4-7999fde7a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data (if not already done)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words for stemming\n",
    "word = \"cooking\"\n",
    "\n",
    "print(f\"stemmed version of {word}: {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585a9a5-4a9f-4a87-8aee-2fd8dec3e769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the Porter Stemmer to stem the documents in our collection\n",
    "\n",
    "stemmed_documents = \n",
    "\n",
    "print(stemmed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7289ec8-9b7a-4fc6-b93e-c689d73f93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the result to a dense array (optional, but easier to read)\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Print the TF-IDF matrix with corresponding words\n",
    "df = pd.DataFrame(dense_matrix, index=[f\"doc{x+1}\" for x in range(len(stemmed_documents))], columns=feature_names)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20466b67-ec86-4fb7-b5ce-e9625648f966",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "Stopwords are **common words** (e.g., *\"the\"*, *\"is\"*, *\"and\"*) that **do not carry significant meaning** and are often removed in text preprocessing for **Natural Language Processing (NLP)** tasks.\n",
    "\n",
    "### Why Remove Stopwords?\n",
    "- They appear frequently but contribute **little to meaning**.\n",
    "- Removing them helps **reduce text size** and **improve model performance** in tasks like search, classification, and sentiment analysis.\n",
    "\n",
    "### Example of Stopwords:\n",
    "- **English:** *\"the\", \"is\", \"in\", \"and\", \"a\", \"to\", \"of\"*\n",
    "- **Spanish:** *\"el\", \"la\", \"y\", \"de\", \"en\"*\n",
    "- Different languages have different sets of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f4861-d48c-41df-9dfc-f79b289a1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ae30e-db4f-4851-b578-dfd0b5ef2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(doc):\n",
    "\n",
    "# Apply stopword removal to each document\n",
    "cleaned_documents = [remove_stopwords(doc) for doc in clean_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b232a0-2c6b-40a5-877e-8667378e1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem documents\n",
    "split_documents = [x.split(\" \") for x in cleaned_documents]\n",
    "stemmed_documents = [\" \".join([stemmer.stem(word) for word in doc]) for doc in split_documents]\n",
    "\n",
    "# Show the cleaned documents\n",
    "for i, doc in enumerate(stemmed_documents):\n",
    "    print(f\"Document {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f927557-d5c5-407e-a8c9-271a95fef663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the result to a dense array (optional, but easier to read)\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Print the TF-IDF matrix with corresponding words\n",
    "df = pd.DataFrame(dense_matrix, index=[f\"doc{x+1}\" for x in range(len(stemmed_documents))], columns=feature_names)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6cd35-79bb-48ca-9c21-92b1242f11d3",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "**Cosine Similarity** measures the similarity between two vectors (words or text documents) by calculating the cosine of the angle between their vector representations. It is commonly used in NLP for comparing document or word similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a3b90-dc06-419c-b132-552909380140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word1 = \"carbonara\"\n",
    "word2 = \"pasta\"\n",
    "\n",
    "word_index1 = vectorizer.vocabulary_.get(word1)\n",
    "word_index2 = vectorizer.vocabulary_.get(word2)\n",
    "\n",
    "if word_index1 is not None and word_index2 is not None:\n",
    "    # Get the TF-IDF vector for the two words\n",
    "    word_tfidf_vector1 = tfidf_matrix[:, word_index1].toarray().reshape(1, -1)  # Reshaping to make it a 2D array (1xN)\n",
    "    word_tfidf_vector2 = tfidf_matrix[:, word_index2].toarray().reshape(1, -1)  # Reshaping to make it a 2D array (1xN)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim_words = cosine_similarity(word_tfidf_vector1, word_tfidf_vector2)\n",
    "\n",
    "    print(f\"Cosine similarity between the word '{word1}' and the word '{word2}': {cosine_sim_words}'\")\n",
    "else:\n",
    "    print(f\"The words are not in the TF-IDF vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762b1d5-bad0-491d-a9e4-d812d34506fc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. what are the two most similar documents in the list? why do you think that is?\n",
    "2. add a new document to the list such that:\n",
    "    - the score of the stem 'egg' in the new document is higher than it is in all other documents;\n",
    "    - cosine similarity between words 'carbonara' and 'pasta' drops below 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b47ff8-c106-41d9-9220-cbe675299c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef9c4aa-7461-42c5-b513-f77fe53b8922",
   "metadata": {},
   "source": [
    "##Â Information Retrieval Task Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9bc932-9d8c-4333-bfcf-ed247ca76b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more recipes to our collection\n",
    "documents += [\n",
    "\n",
    "    \"\"\"\n",
    "    To prepare amatriciana you need pasta, guanciale, peeled tomatoes, roman pecorino cheese.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Ingredients for amatriciana: guanciale, pasta, tomato sauce, pecorino romano DOP and black pepper.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Pasta all'amatriciana - ingredients: guanciale, white wine, peeled tomatoes, pecorino, pasta.\n",
    "    \"\"\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc855ea-bca8-4839-a337-09cc2b6e97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents = [re.sub(r\"[^\\w\\s]\", \"\", x.replace(\"\\n\", \"\").lower()) for x in documents]\n",
    "clean_documents = [remove_stopwords(doc) for doc in clean_documents]\n",
    "\n",
    "split_documents = [x.split(\" \") for x in clean_documents]\n",
    "stemmed_documents = [\" \".join([stemmer.stem(word) for word in doc]) for doc in split_documents]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the result to a dense array (optional, but easier to read)\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Print the TF-IDF matrix with corresponding words\n",
    "df = pd.DataFrame(dense_matrix, index=[f\"doc{x+1}\" for x in range(len(stemmed_documents))], columns=feature_names)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b958b42-ad18-4b86-88e7-dae0709c1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how do I prepare amatriciana pasta?\"\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e0e25-4423-4199-b153-fc0a3d03d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess query and apply tf-idf\n",
    "clean_query = \n",
    "split_query = \n",
    "stemmed_query = \n",
    "\n",
    "vector_query = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467dc04-a98b-4d1b-b701-1f6ac74aed07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rank our documents based on similarity with respect to our query\n",
    "rank = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6a1ba-2bd6-4912-b5f6-d7feb27926d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top4 relevant documents\n",
    "rank = \n",
    "\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3baf5-3e21-428c-89af-5c68569b23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate precision and recall of our search task. What can we say about these metrics?\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(f\"The precision of our retrieval task is {}%\")\n",
    "print(f\"The recall of our retrieval task is {}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57686046-7f40-4adf-bc39-c1f4b6fc9e3c",
   "metadata": {},
   "source": [
    "# Tokenization in Text Mining\n",
    "\n",
    "## What is Tokenization?\n",
    "Tokenization is the process of breaking down text into smaller units called **tokens**. These tokens can be **words**, **sentences**, or **subwords**, depending on the approach used.\n",
    "\n",
    "## Why is Tokenization Important?\n",
    "- It is the first step in **Natural Language Processing (NLP)**.\n",
    "- Helps in **text analysis, search engines, chatbots, and machine learning models**.\n",
    "- Makes it easier to process and analyze textual data.\n",
    "\n",
    "## Types of Tokenization:\n",
    "1. **Word Tokenization**: Splitting text into individual words.\n",
    "2. **Sentence Tokenization**: Splitting text into sentences.\n",
    "3. **Subword Tokenization**: Splitting words into smaller meaningful parts (used in deep learning models like BERT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d7aa5-43ff-41f7-a02f-9297dd17f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"One cannot be too careful with words, they change their minds just as people do.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "print(f\"text: {text}\")\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "### Code Cell: Sentence Tokenization with NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"War is peace. Freedom is slavery. Ignorance is strength.\"\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(f\"text: {text}\")\n",
    "print(\"Sentence Tokens:\", sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fc343-6ee8-48f5-af24-264f1f0738bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Tokenization is crucial for modern text-mining models.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Subword Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab03c3-31b3-4369-914d-230a310d9f88",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "The **Attention Mechanism** is a technique that allows models to focus on specific parts of an input sequence when making predictions, rather than processing the entire sequence uniformly. This mechanism is especially useful in tasks like machine translation, summarization, and other sequence-to-sequence tasks.\n",
    "\n",
    "In traditional sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM), each input is processed sequentially. The attention mechanism enhances this by enabling the model to focus on important words or tokens from the input sequence while making predictions at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dfa93-a3de-4e25-8002-bb50ccc228fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from bertviz import head_view\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df4e6d-52d6-4fc4-b8fc-b05346949d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The kid was playing with the ball. Then he threw it at his grandma.\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c8119-d5c0-42c1-9b7d-1801689cc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a vanilla BERT-base model. \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff024aca-433d-4761-9d2f-e4deec12486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(text)\n",
    "inputs = torch.tensor(tokens).unsqueeze(0) # unsqueeze changes the shape from (20,) -> (1, 20)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb081fd7-8145-4e9a-a969-3ab49211b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = model(inputs, output_attentions=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf42a4-bb81-4550-b030-c679e9513486",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_list = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "head_view(attention, tokens_as_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b07bb6-7d94-49b3-a3b4-8ea39a6dfe41",
   "metadata": {},
   "source": [
    "# Transformer Architectures: Encoder-Decoder, BERT, GPT\n",
    "\n",
    "Transformers are a type of neural network architecture that revolutionized natural language processing (NLP). They rely on the **attention mechanism** to process input data in parallel (unlike older sequential models like RNNs), making them faster and more effective for tasks like translation, summarization, and text generation.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components of Transformers\n",
    "\n",
    "1. **Encoder**:  \n",
    "   - The encoder processes the input data (e.g., a sentence) and converts it into a set of hidden representations (vectors) that capture the meaning of the input.  \n",
    "   - It uses **self-attention** to understand relationships between all words in the input, regardless of their distance from each other.\n",
    "\n",
    "2. **Decoder**:  \n",
    "   - The decoder generates output data (e.g., a translated sentence) based on the encoder's hidden representations.  \n",
    "   - It also uses self-attention but adds an extra step to focus on the encoder's output, ensuring the generated output is aligned with the input.\n",
    "  \n",
    "\n",
    "![Alt text](https://aiml.com/wp-content/uploads/2023/09/Annotated-Transformers-Architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Encoder-Decoder Architecture\n",
    "\n",
    "The original Transformer model (introduced in the paper *\"Attention is All You Need\"*) uses both an encoder and a decoder. This architecture is ideal for **sequence-to-sequence tasks**, such as:\n",
    "- **Machine Translation**: Translating a sentence from one language to another.\n",
    "- **Text Summarization**: Generating a shorter version of a long document.\n",
    "\n",
    "---\n",
    "\n",
    "## BERT vs. GPT: Two Popular Transformer Variants\n",
    "\n",
    "While both BERT and GPT are based on the Transformer architecture, they are designed for different purposes:\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **Architecture**: Uses only the **encoder** part of the Transformer.\n",
    "- **Key Feature**: Bidirectional attention, meaning it looks at both the left and right context of a word simultaneously. This makes BERT great for understanding the meaning of words in context.\n",
    "- **Use Cases**:  \n",
    "  - Sentence classification (e.g., spam detection).  \n",
    "  - Question answering (e.g., finding answers in a paragraph).  \n",
    "  - Named entity recognition (e.g., identifying names, dates, or locations in text).\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer)\n",
    "- **Architecture**: Uses only the **decoder** part of the Transformer.\n",
    "- **Key Feature**: Unidirectional attention, meaning it processes text from left to right. This makes GPT excellent for generating coherent and contextually relevant text.\n",
    "- **Use Cases**:  \n",
    "  - Text generation (e.g., writing essays, stories, or code).  \n",
    "  - Chatbots and conversational AI.  \n",
    "  - Autocompletion (e.g., suggesting the next word in a sentence).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Feature               | BERT (Encoder)                     | GPT (Decoder)                     |\n",
    "|-----------------------|------------------------------------|------------------------------------|\n",
    "| **Attention**         | Bidirectional (looks at both sides)| Unidirectional (left-to-right)    |\n",
    "| **Use Case**          | Understanding text                 | Generating text                   |\n",
    "| **Example Tasks**     | Question answering, classification | Text generation, chatbots         |\n",
    "\n",
    "Transformers have become the backbone of modern NLP, and understanding their architecture and variants (like BERT and GPT) is key to leveraging their power for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c680d-e6af-4269-82db-ffdd3a4f4227",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://www.ibm.com/think/insights/managing-unstructured-data\n",
    "- https://machinelearningmastery.com/gentle-introduction-bag-words-model/ (Bag of Words deep dive)\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf (tf-idf wiki)\n",
    "- https://www.kopp-online-marketing.com/what-is-bm25 (more powerful representation with bm25)\n",
    "- https://spotintelligence.com/2023/09/07/vector-space-model/ (Vector space model for documents)\n",
    "\n",
    "- https://arxiv.org/abs/1706.03762 (Attention is all you need - first paper on transformers)\n",
    "- https://h2o.ai/wiki/bert/ (BERT deep dive)\n",
    "- https://www.ibm.com/think/topics/gpt (GPT deep dive)\n",
    "\n",
    "### Other NLP Tasks\n",
    "- https://www.techtarget.com/searchbusinessanalytics/definition/opinion-mining-sentiment-mining (Sentiment Analysis)\n",
    "- https://huggingface.co/tasks/text-classification (Text Classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
