{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973bb492-93a8-4415-8b04-113be960f597",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING\n",
    "**Transfer learning** (o **apprendimento per trasferimento**) è una tecnica nel **machine learning** e nel **deep learning** in cui un modello pre-addestrato su un determinato compito viene riutilizzato come punto di partenza per un nuovo compito. In altre parole, si sfruttano le conoscenze acquisite da un modello su un problema (spesso in un dominio simile o correlato) per migliorare le prestazioni su un nuovo problema, riducendo il tempo di addestramento e migliorando l'efficienza.\n",
    "\n",
    "### Come funziona il **Transfer Learning**?\n",
    "\n",
    "L'idea principale del transfer learning è che un modello addestrato su un ampio dataset per un compito complesso può imparare rappresentazioni generali dei dati (ad esempio, caratteristiche di immagini) che possono essere utili anche per un altro compito, spesso in un dominio diverso o con dati limitati.\n",
    "\n",
    "Ecco come funziona tipicamente:\n",
    "\n",
    "1. **Pre-addestramento su un grande dataset**:\n",
    "   - Un modello viene addestrato su un **grande dataset** con molte immagini, testi o altri dati (ad esempio, **ImageNet** per le immagini). Questo addestramento consente al modello di imparare caratteristiche generali dei dati (come bordi, forme, texture per le immagini).\n",
    "   \n",
    "2. **Riutilizzo del modello pre-addestrato**:\n",
    "   - Il modello pre-addestrato viene quindi **riutilizzato** per un altro compito, dove si ha meno data disponibile. L'idea è che le conoscenze acquisite nelle prime fasi (come i filtri convoluzionali in una rete neurale) possano essere utili anche per il nuovo compito.\n",
    "\n",
    "3. **Fine-tuning**:\n",
    "   - Il modello pre-addestrato viene adattato al nuovo compito attraverso una fase di **fine-tuning**. Qui, si \"ri-addestra\" il modello su un nuovo dataset, ma invece di partire da zero, si parte dai pesi già ottimizzati nelle prime fasi. Il fine-tuning può essere fatto su tutti i livelli della rete o solo su alcuni livelli, a seconda della somiglianza tra il compito originale e il nuovo compito.\n",
    "\n",
    "### Tipi di **Transfer Learning**\n",
    "\n",
    "Ci sono vari modi di applicare il transfer learning, a seconda di quanto si desidera adattare il modello pre-addestrato al nuovo compito:\n",
    "\n",
    "1. **Utilizzo del modello pre-addestrato come estrattore di caratteristiche**:\n",
    "   - In questo caso, si prende un modello pre-addestrato e lo si usa come un **estrattore di caratteristiche**. Solo le **caratteristiche apprese** dal modello vengono utilizzate per alimentare un altro modello (ad esempio, un classificatore semplice come una regressione logistica o una SVM).\n",
    "   - Esempio: Si utilizza una CNN pre-addestrata per estrarre caratteristiche da immagini, quindi si collega una rete neurale fully connected per la classificazione finale.\n",
    "\n",
    "2. **Fine-tuning completo o parziale**:\n",
    "   - In questo approccio, **tutti** o solo alcuni strati del modello pre-addestrato vengono ri-addestrati per adattarsi meglio al nuovo compito. Il fine-tuning completo implica l'addestramento di tutti i pesi del modello, mentre il fine-tuning parziale implica il blocco di alcuni strati (generalmente quelli più vicini all'input) mentre si addestrano solo gli strati superiori (più vicini all'output).\n",
    "\n",
    "3. **Addestramento del modello da zero con pre-training su un compito simile**:\n",
    "   - Si inizia da un modello pre-addestrato su un compito simile, ma si addestra tutto da zero sul nuovo compito. Questa tecnica viene usata quando c'è una certa affinità tra i compiti, ma è necessario un addestramento completo.\n",
    "\n",
    "### Perché il **Transfer Learning** è utile?\n",
    "\n",
    "1. **Riduzione del tempo di addestramento**:\n",
    "   - Addestrare un modello da zero richiede enormi quantità di dati e risorse computazionali. Con il transfer learning, puoi risparmiare molto tempo, sfruttando le conoscenze acquisite dal modello su un compito precedente.\n",
    "   \n",
    "2. **Efficiente con pochi dati**:\n",
    "   - In molti scenari, non si ha a disposizione un ampio dataset per addestrare un modello da zero. Il transfer learning permette di ottenere buoni risultati anche con pochi dati, poiché il modello ha già appreso molte caratteristiche utili da un altro dominio.\n",
    "\n",
    "3. **Miglioramento delle prestazioni**:\n",
    "   - Utilizzando un modello pre-addestrato su un dataset molto grande (come ImageNet per immagini o BERT per il linguaggio naturale), il modello può trasferire conoscenze generali che migliorano le prestazioni sul nuovo compito.\n",
    "\n",
    "4. **Adattamento a nuovi domini**:\n",
    "   - Il transfer learning è utile anche quando si cerca di applicare un modello a un dominio leggermente diverso da quello su cui è stato addestrato. Ad esempio, un modello addestrato per riconoscere animali in immagini può essere adattato a riconoscere piante o altri oggetti in immagini.\n",
    "\n",
    "### Esempi di **Transfer Learning**\n",
    "\n",
    "1. **Riconoscimento delle immagini**:\n",
    "   - Le CNN pre-addestrate su dataset come **ImageNet** (che contiene milioni di immagini etichettate in centinaia di categorie) possono essere facilmente adattate per compiti di riconoscimento delle immagini in nuovi domini, come il riconoscimento di malattie nelle immagini mediche o il riconoscimento di difetti nei prodotti industriali.\n",
    "\n",
    "2. **NLP (Natural Language Processing)**:\n",
    "   - Modelli come **BERT**, **GPT**, o **T5** sono stati pre-addestrati su enormi corpus di testo (come Wikipedia, libri, articoli) e possono essere adattati a compiti specifici di NLP, come la classificazione del testo, l'analisi del sentiment, la traduzione automatica, ecc.\n",
    "\n",
    "3. **Riconoscimento vocale**:\n",
    "   - Un modello di riconoscimento vocale pre-addestrato, come **DeepSpeech**, può essere utilizzato per migliorare la trascrizione automatica in lingue o contesti specifici, adattandosi a diversi accenti, slang, o ambienti.\n",
    "\n",
    "### Come applicare il **Transfer Learning**?\n",
    "\n",
    "1. **Scegliere un modello pre-addestrato**: Ad esempio, una CNN come **VGG16**, **ResNet** o **Inception** per il riconoscimento delle immagini, o modelli NLP come **BERT** per l'elaborazione del linguaggio naturale.\n",
    "   \n",
    "2. **Caricare il modello**: Molti framework come **TensorFlow** e **PyTorch** offrono modelli pre-addestrati che puoi scaricare facilmente.\n",
    "\n",
    "3. **Personalizzare il modello**: Sostituire l'ultimo strato (o più strati) del modello pre-addestrato con uno più adatto al tuo compito specifico. Ad esempio, sostituire l'ultimo strato di classificazione con un classificatore che abbia il numero corretto di classi.\n",
    "\n",
    "4. **Fine-tuning**: Addestrare il modello sui tuoi dati specifici. Puoi scegliere di addestrare tutti i pesi o solo quelli degli strati superiori.\n",
    "\n",
    "### Conclusione\n",
    "\n",
    "Il **transfer learning** è una potente tecnica che consente di sfruttare modelli pre-addestrati per risparmiare tempo, risorse computazionali e migliorare le prestazioni in compiti complessi, specialmente quando i dati sono scarsi. È particolarmente utile in applicazioni come il riconoscimento delle immagini, l'elaborazione del linguaggio naturale e molte altre aree dove è possibile riutilizzare conoscenze acquisite da modelli precedenti per risolvere problemi nuovi o simili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5fa63-5854-4f4e-b7c8-8ae19183430f",
   "metadata": {},
   "source": [
    "# RETI CONVOLUZIONALI \n",
    "Le **reti neurali convoluzionali (CNN)**, o **Convolutional Neural Networks**, sono una classe speciale di **reti neurali artificiali** progettate per trattare dati che hanno una struttura **spaziale o grigliata**. Le **CNN** sono particolarmente efficaci per compiti di **elaborazione delle immagini**, ma possono essere utilizzate anche per altre tipologie di dati come il **video**, il **suono** e il **testo**. \n",
    "\n",
    "Le CNN sono composte da una serie di strati (o layer) che eseguono operazioni di **convoluzione** e **pooling**, seguite da strati completamente connessi (fully connected) per generare previsioni o classificazioni.\n",
    "\n",
    "### Caratteristiche principali delle **CNN**:\n",
    "\n",
    "#### 1. **Strati Convoluzionali (Convolutional Layers)**\n",
    "Gli **strati convoluzionali** sono il cuore delle CNN e sono progettati per **estrarre caratteristiche locali** dai dati di input. Un'operazione di **convoluzione** applica un **filtro** (o **kernel**) sull'input per produrre una mappa di attivazione. Ogni filtro è responsabile del rilevamento di una specifica caratteristica nell'immagine, come bordi, angoli, texture, ecc.\n",
    "\n",
    "- **Filtro (kernel)**: È una matrice di numeri che scorre sull'immagine, moltiplicando i valori del filtro per i valori dell'immagine stessa e sommando i risultati. In questo modo, il filtro estrae caratteristiche locali dall'immagine.\n",
    "  \n",
    "- **Mappa di attivazione**: È il risultato della convoluzione, che rappresenta le attivazioni delle caratteristiche rilevate dal filtro.\n",
    "\n",
    "- **Indipendenza dalla posizione**: Grazie alla convoluzione, le CNN sono in grado di rilevare le stesse caratteristiche in qualsiasi punto dell'immagine (traslazione invariance), un aspetto fondamentale per il riconoscimento di oggetti.\n",
    "\n",
    "#### 2. **Strati di Pooling (Pooling Layers)**\n",
    "Il **pooling** è un'operazione che riduce la **dimensione spaziale** della mappa di attivazione, mantenendo solo le informazioni più rilevanti. Il tipo più comune di pooling è il **max pooling**, in cui viene scelto il valore massimo in ogni finestra locale dell'immagine. Questo aiuta a ridurre il numero di parametri, rendendo il modello più efficiente e riducendo il rischio di overfitting.\n",
    "\n",
    "- **Max Pooling**: Seleziona il valore massimo da una finestra di dimensioni fisse (ad esempio, 2x2).\n",
    "- **Average Pooling**: Seleziona il valore medio in una finestra locale.\n",
    "\n",
    "Il pooling riduce la risoluzione spaziale delle attivazioni, il che significa che il modello diventa più veloce e meno soggetto a overfitting, mentre preserva le caratteristiche salienti.\n",
    "\n",
    "#### 3. **Strati Completamente Connessi (Fully Connected Layers)**\n",
    "Dopo aver applicato la convoluzione e il pooling, le informazioni estratte vengono \"appiattite\" in un **vettore unidimensionale** attraverso l'operazione di **flattening**. Questo vettore viene quindi passato a uno o più **strati completamente connessi**, che combinano le caratteristiche apprese per fare la predizione finale (ad esempio, la classificazione di un'immagine in una delle categorie predefinite).\n",
    "\n",
    "- **Strati Fully Connected**: Ogni neurone in uno strato fully connected è collegato a tutti i neuroni dello strato precedente. Questo permette al modello di combinare le informazioni apprese in precedenza e fare una decisione finale.\n",
    "\n",
    "#### 4. **Funzione di Attivazione**\n",
    "Come in tutte le reti neurali, le CNN utilizzano **funzioni di attivazione** non lineari (come **ReLU**, **Sigmoid**, **Tanh**) dopo ogni strato per introdurre non linearità nel modello. La funzione di attivazione permette alla rete di apprendere **comportamenti complessi** e **pattern non lineari** nei dati.\n",
    "\n",
    "### Come funzionano le **CNN**?\n",
    "\n",
    "Le CNN sono progettate per rilevare **caratteristiche locali** in un'immagine e **combinare** queste caratteristiche per riconoscere oggetti complessi. Ecco un esempio di come funziona una CNN:\n",
    "\n",
    "1. **Convoluzione**: Un'immagine (ad esempio, di dimensioni 224x224x3) viene passata a uno strato convoluzionale, dove vari filtri (come bordi orizzontali, bordi verticali, angoli, texture) vengono applicati sull'immagine per estrarre caratteristiche locali.\n",
    "   \n",
    "2. **Pooling**: Successivamente, si applica un'operazione di pooling per ridurre la dimensione spaziale dell'immagine e mantenere solo le informazioni più rilevanti.\n",
    "\n",
    "3. **Ripetizione (strati profondi)**: L'immagine passa attraverso più strati convoluzionali e di pooling, dove vengono apprese caratteristiche sempre più astratte, come oggetti complessi, facce, e strutture riconoscibili.\n",
    "\n",
    "4. **Flattening**: Dopo diversi strati convoluzionali e di pooling, la mappa di attivazione viene \"appiattita\" in un vettore 1D.\n",
    "\n",
    "5. **Strati Fully Connected**: Il vettore appiattito passa agli strati completamente connessi, che combinano le informazioni per fare una previsione finale, come ad esempio la classificazione dell'immagine in una delle categorie predefinite (es. cane, gatto, automobile, ecc.).\n",
    "\n",
    "### Perché le **CNN** sono efficaci per le immagini?\n",
    "\n",
    "Le CNN sono molto potenti per il riconoscimento delle immagini e altre applicazioni legate alla visione artificiale grazie a vari motivi:\n",
    "\n",
    "1. **Rilevamento di caratteristiche locali**: Le CNN sono in grado di rilevare bordi, angoli, forme e altre caratteristiche locali che sono fondamentali per riconoscere oggetti nelle immagini.\n",
    "   \n",
    "2. **Condivisione dei pesi**: I filtri vengono applicati su tutta l'immagine, il che significa che gli stessi pesi sono usati per rilevare caratteristiche in diverse regioni dell'immagine. Questo riduce il numero di parametri e rende il modello più efficiente.\n",
    "   \n",
    "3. **Invarianza alla traslazione**: Le CNN sono robuste a piccoli spostamenti degli oggetti nell'immagine. Se un oggetto si sposta, il filtro sarà comunque in grado di rilevarlo in una nuova posizione.\n",
    "\n",
    "4. **Gerarchia di caratteristiche**: Le CNN possono apprendere una **gerarchia di caratteristiche**: strati superficiali rilevano caratteristiche di basso livello (come bordi e angoli), mentre strati più profondi combinano queste caratteristiche per riconoscere oggetti complessi.\n",
    "\n",
    "### Esempio di utilizzo delle **CNN**:\n",
    "\n",
    "1. **Classificazione delle immagini**: Le CNN sono ampiamente utilizzate per la classificazione delle immagini, ad esempio per distinguere tra immagini di cani, gatti, automobili, ecc.\n",
    "   \n",
    "2. **Segmentazione semantica**: In questo caso, le CNN sono utilizzate per etichettare ogni pixel di un'immagine con una classe, utile per applicazioni come la medicina (ad esempio, segmentare le immagini mediche per identificare tumori).\n",
    "   \n",
    "3. **Riconoscimento facciale**: Le CNN vengono utilizzate per rilevare e riconoscere i volti nelle immagini.\n",
    "\n",
    "4. **Autonomous driving**: Le CNN vengono utilizzate nei veicoli autonomi per riconoscere segnali stradali, pedoni e altri veicoli nelle immagini catturate dalle telecamere.\n",
    "\n",
    "### Conclusione\n",
    "\n",
    "Le **reti neurali convoluzionali (CNN)** sono una classe fondamentale di modelli di deep learning, che eccellono nelle attività di elaborazione delle immagini, grazie alla loro capacità di rilevare caratteristiche locali e costruire rappresentazioni gerarchiche di alta qualità dei dati. Utilizzando operazioni di **convoluzione** e **pooling**, le CNN sono in grado di ridurre la complessità del modello e aumentare l'efficienza, mentre gli strati completamente connessi finali forniscono la capacità di fare previsioni o classificazioni. Le CNN sono alla base di molte delle applicazioni moderne di **visione artificiale** e **riconoscimento delle immagini**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7342dc-10fa-4cab-b1f4-4ae10e2a9a87",
   "metadata": {},
   "source": [
    "# ATTENTION E CONSECUTIVITA'\n",
    "\n",
    "L'**Attention** e la **consecutività di una sequenza** sono concetti fondamentali nell'elaborazione del linguaggio naturale (NLP) e nelle architetture di deep learning come i **Transformer**. Vediamo di spiegare entrambi in dettaglio.\n",
    "\n",
    "### **1. Cos'è l'Attention?**\n",
    "\n",
    "**Attention** è un meccanismo che permette ai modelli di deep learning di **focalizzarsi su parti specifiche dei dati in ingresso** mentre elaborano un'informazione, piuttosto che trattare tutto allo stesso modo. È una tecnica che simula il modo in cui gli esseri umani **prestano attenzione a determinati dettagli** durante un'attività, ignorando quelli che sono meno rilevanti.\n",
    "\n",
    "Nel contesto del **NLP** (elaborazione del linguaggio naturale) e, in particolare, nei modelli basati su **Transformer**, il meccanismo di **attention** permette al modello di pesare in modo differente le parole in una sequenza mentre elabora ciascun elemento.\n",
    "\n",
    "#### **Meccanismo di Attention**\n",
    "Il meccanismo di attention calcola un **peso** per ogni elemento della sequenza in base alla sua **pertinenza** rispetto agli altri elementi. Ad esempio, quando il modello sta cercando di tradurre una frase, potrebbe \"prestare attenzione\" a determinate parole che sono più rilevanti per una traduzione accurata.\n",
    "\n",
    "Il concetto chiave in un meccanismo di attention è che, invece di trattare ogni parola nella sequenza in modo uguale, il modello **assegna un peso diverso a ciascuna parola**. Questo è utile soprattutto in compiti come la **traduzione automatica**, la **generazione di testo** o la **comprensione del linguaggio**.\n",
    "\n",
    "#### **Scaled Dot-Product Attention** (Attention Scaled a Prodotto di Scalare)\n",
    "Un'implementazione popolare del meccanismo di attention è il **Scaled Dot-Product Attention**. Funziona nel seguente modo:\n",
    "\n",
    "- **Query (Q)**: Un vettore che rappresenta la parola di interesse (o il punto attuale della sequenza).\n",
    "- **Key (K)**: Un vettore che rappresenta ogni parola della sequenza.\n",
    "- **Value (V)**: Un vettore che rappresenta le informazioni associate ad ogni parola.\n",
    "\n",
    "L'attenzione calcola un punteggio tra la **query** e le **key** e utilizza quel punteggio per ottenere un \"peso\" per ciascun valore **V**. Questo punteggio determina quanto ciascun valore contribuisce all'output finale.\n",
    "\n",
    "L'output dell'attenzione è una **combinazione pesata dei valori (V)**, dove i pesi sono determinati dalla somiglianza tra la query e le chiavi.\n",
    "\n",
    "### **2. Cos'è la Consecutività di una Sequenza?**\n",
    "\n",
    "La **consecutività di una sequenza** è un concetto che si riferisce alla relazione tra gli elementi di una sequenza. In particolare, si tratta di come il modello considera la **dipendenza temporale** tra gli elementi, come nelle parole di una frase o nei passaggi di una sequenza di eventi.\n",
    "\n",
    "Nel contesto del NLP, molte architetture tradizionali come le **Reti Neurali Ricorrenti (RNN)** o le **Long Short-Term Memory (LSTM)** si concentrano molto sulla **consecutività** delle sequenze, dove l'ordine degli elementi è cruciale. Ad esempio, nelle RNN e LSTM, **l'output di un passo temporale dipende direttamente dall'input e dallo stato precedente** (quindi, c'è una dipendenza sequenziale).\n",
    "\n",
    "Tuttavia, **i Transformer** (e quindi il meccanismo di **Attention**) trattano la **sequenza nel suo complesso** e non dipendono necessariamente da un ordine lineare o temporale, permettendo di **elaborare tutta la sequenza contemporaneamente**.\n",
    "\n",
    "### **3. Transformer e Attention**\n",
    "\n",
    "Nei modelli basati su **Transformer**, come **BERT** o **GPT**, l'attenzione è utilizzata per **gestire la consecutività** in modo molto più flessibile ed efficiente rispetto alle RNN/LSTM. La **sequenza di parole** viene trattata **tutta insieme**, e il modello applica il meccanismo di attention per determinare quali parole \"prestare attenzione\" in base alla loro rilevanza per ciascuna parola corrente.\n",
    "\n",
    "Nei **Transformer**, non c'è un'elaborazione sequenziale come nelle RNN. Ogni parola nella sequenza può \"guardare\" simultaneamente tutte le altre parole e \"prestare attenzione\" a quelle più rilevanti. Questo approccio consente al modello di **catturare dipendenze a lungo raggio** tra le parole in modo molto più efficiente.\n",
    "\n",
    "#### **Self-Attention**\n",
    "Nel **self-attention**, che è una parte cruciale del Transformer, ogni parola della sequenza prende in considerazione tutte le altre parole per determinare quanto \"attenti\" dovrebbero essere rispetto a ciascuna di esse. Questo è molto utile per catturare **relazioni complesse** tra le parole, che potrebbero non essere immediatamente evidenti in una sequenza lineare.\n",
    "\n",
    "### **Perché l'Attention Funziona Bene per la Consecutività?**\n",
    "\n",
    "Il meccanismo di **attention** è particolarmente potente per gestire la consecutività nelle sequenze perché:\n",
    "\n",
    "1. **Dipendenza a lungo raggio**: L'attenzione consente di catturare relazioni a lungo termine tra le parole, indipendentemente dalla loro posizione nella sequenza. Questo è difficile da fare con le RNN, che tendono a \"dimenticare\" informazioni da lontano nel tempo.\n",
    "  \n",
    "2. **Parallelizzazione**: A differenza delle RNN/LSTM, che devono processare la sequenza passo dopo passo, i **Transformer** possono trattare tutte le parole contemporaneamente, applicando l'attenzione in parallelo. Questo rende l'elaborazione più veloce e più scalabile.\n",
    "  \n",
    "3. **Flessibilità nelle dipendenze**: L'attenzione consente di modellare dipendenze non lineari tra gli elementi di una sequenza, migliorando la capacità del modello di catturare connessioni complesse che potrebbero non essere consecutive ma comunque rilevanti (ad esempio, una parola in fondo alla frase che dipende da una all'inizio della frase).\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "In sintesi:\n",
    "\n",
    "- **Attention** è un meccanismo che permette di focalizzarsi su parti rilevanti di una sequenza durante l'elaborazione. Utilizza un sistema di pesi per determinare quali elementi di una sequenza sono più importanti per una determinata attività.\n",
    "  \n",
    "- La **consecutività di una sequenza** riguarda l'ordine in cui gli elementi vengono trattati, ed è tradizionalmente gestita da modelli come le RNN o LSTM, ma nei modelli Transformer viene trattata in modo più flessibile attraverso il meccanismo di attention.\n",
    "\n",
    "Il meccanismo di **attention** è stato un grande passo avanti nel miglioramento delle prestazioni in vari compiti di **elaborazione del linguaggio naturale**, permettendo una gestione più dinamica ed efficiente delle sequenze rispetto ai modelli precedenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b253ce-d47d-4ae3-9ad9-dd613829445e",
   "metadata": {},
   "source": [
    "# GPU E TPU\n",
    "\n",
    "**TPU** (Tensor Processing Unit) e **GPU** (Graphics Processing Unit) sono due tipi di **unità di elaborazione specializzate** che accelerano i calcoli necessari per l'addestramento e l'inferenza di modelli di **deep learning**. Entrambe sono progettate per eseguire operazioni matematiche ad alte prestazioni, ma sono ottimizzate per scopi differenti. Vediamo nel dettaglio cosa sono e come funzionano.\n",
    "\n",
    "### **GPU (Graphics Processing Unit)**\n",
    "\n",
    "La **GPU** è un tipo di processore progettato originariamente per gestire i calcoli grafici necessari per il rendering delle immagini, come nel caso di videogiochi, realtà virtuale e altre applicazioni grafiche. Tuttavia, grazie alla sua capacità di eseguire in parallelo migliaia di operazioni, la GPU è diventata uno strumento estremamente potente anche per il **machine learning** e il **deep learning**.\n",
    "\n",
    "#### **Caratteristiche principali della GPU:**\n",
    "1. **Elaborazione parallela**: Le GPU sono ottimizzate per eseguire milioni di calcoli simultaneamente. Mentre una CPU (Central Processing Unit) ha generalmente pochi core, le GPU ne hanno centinaia o migliaia, permettendo di eseguire grandi quantità di operazioni matematiche contemporaneamente.\n",
    "   \n",
    "2. **Ottimizzazione per operazioni matriciali**: Le operazioni comuni nel deep learning, come la moltiplicazione di matrici (che è alla base delle operazioni nei modelli di reti neurali), sono molto efficienti sulle GPU. Questo le rende ideali per addestrare modelli di deep learning.\n",
    "\n",
    "3. **Flessibilità**: Le GPU sono progettate per supportare una varietà di calcoli, non solo per il rendering grafico, ma anche per calcoli generali, inclusi quelli relativi a algoritmi di machine learning.\n",
    "\n",
    "4. **Librerie per il deep learning**: Le GPU sono supportate da numerose librerie per deep learning, come **TensorFlow**, **PyTorch**, e **CUDA** (un'architettura sviluppata da NVIDIA per sfruttare le GPU).\n",
    "\n",
    "#### **Quando usare una GPU:**\n",
    "Le GPU sono molto usate in compiti di **deep learning** come:\n",
    "- Addestramento di modelli di reti neurali (CNN, RNN, Transformer).\n",
    "- Inferenza (predizione) in modelli di machine learning.\n",
    "- Elaborazione di grandi quantità di dati, come l'analisi di immagini, video, e testo.\n",
    "\n",
    "Le GPU sono versatili e possono essere utilizzate per molti tipi di compiti di calcolo intensivo, ma sono particolarmente efficaci nel **deep learning**.\n",
    "\n",
    "### **TPU (Tensor Processing Unit)**\n",
    "\n",
    "La **TPU** è un tipo di acceleratore hardware sviluppato da **Google** specificamente per ottimizzare le operazioni di **machine learning**, in particolare quelle legate al framework **TensorFlow**. Mentre le GPU sono versatili e possono essere utilizzate per una varietà di calcoli, le TPU sono progettate per **eseguire in modo super efficiente operazioni di deep learning**, come le **moltiplicazioni di matrici** e altre operazioni comuni nei modelli di machine learning.\n",
    "\n",
    "#### **Caratteristiche principali delle TPU:**\n",
    "\n",
    "1. **Ottimizzazione per il deep learning**: Le TPU sono progettate per accelerare in modo massiccio operazioni di **tensorizzazione** (lavorare con tabelle di dati multidimensionali). Le TPU sono altamente specializzate per gestire operazioni di **moltiplicazione di matrici** e altre operazioni di algebra lineare, che sono centrali nel deep learning.\n",
    "\n",
    "2. **Alta efficienza per modelli di TensorFlow**: Google ha progettato le TPU per funzionare al meglio con il framework **TensorFlow**, che è uno degli strumenti di deep learning più popolari. Le TPU sono in grado di gestire enormi quantità di dati molto più velocemente rispetto alle GPU.\n",
    "\n",
    "3. **Architettura specializzata**: A differenza delle GPU, che sono progettate per essere flessibili e gestire una varietà di operazioni, le TPU sono progettate con una **architettura altamente specializzata** che accelera solo determinati tipi di calcoli, in particolare quelli richiesti dal deep learning.\n",
    "\n",
    "4. **Scalabilità**: Le TPU sono progettate per scalare facilmente in grandi **cluster** di calcolo, il che le rende ideali per addestrare modelli di deep learning molto complessi su enormi dataset.\n",
    "\n",
    "5. **Velocità e efficienza energetica**: Le TPU sono ottimizzate per essere estremamente veloci ed efficienti dal punto di vista energetico, il che le rende utili per addestrare modelli di deep learning molto grandi, riducendo al contempo i costi operativi.\n",
    "\n",
    "#### **Quando usare una TPU:**\n",
    "Le TPU sono ideali per:\n",
    "- Addestramento di modelli di deep learning molto grandi e complessi.\n",
    "- Inferenza in tempo reale su modelli di machine learning, soprattutto quando si usano framework come **TensorFlow**.\n",
    "- Situazioni che richiedono **alta velocità** e **alta efficienza energetica**.\n",
    "\n",
    "#### **Dove si usano le TPU?**\n",
    "Le TPU sono disponibili principalmente tramite il **Google Cloud**. Google offre servizi di calcolo basati su TPU che possono essere utilizzati per addestrare e fare inferenza con modelli di machine learning. Le TPU sono più **specializzate** rispetto alle GPU e sono progettate per essere utilizzate in ambienti di **cloud computing**.\n",
    "\n",
    "### **Differenze principali tra GPU e TPU**\n",
    "\n",
    "| **Caratteristica**          | **GPU**                               | **TPU**                                |\n",
    "|-----------------------------|---------------------------------------|----------------------------------------|\n",
    "| **Tipo di operazioni**      | Versatile, adatto per una varietà di calcoli (grafica, machine learning, ecc.) | Ottimizzato per operazioni di deep learning e algebra lineare |\n",
    "| **Uso principale**          | Deep learning, giochi, elaborazione grafica, simulazioni scientifiche | Deep learning, principalmente con TensorFlow |\n",
    "| **Prestazioni**             | Elevate prestazioni per operazioni parallele generali | Prestazioni superiori per calcoli di deep learning |\n",
    "| **Flessibilità**            | Molto flessibile, adatto a una varietà di applicazioni | Specializzato in deep learning (soprattutto TensorFlow) |\n",
    "| **Scalabilità**             | Scalabile, ma richiede molta memoria | Scalabilità altamente ottimizzata per applicazioni distribuite |\n",
    "| **Costo e accessibilità**   | Disponibile in molteplici piattaforme, ma più costosa per uso prolungato | Disponibile principalmente nel cloud Google, costi variabili |\n",
    "\n",
    "### **Quando usare GPU o TPU?**\n",
    "\n",
    "- **Usa una GPU** se:\n",
    "  - Hai bisogno di un dispositivo versatile per vari tipi di calcoli, non solo per il deep learning.\n",
    "  - Lavori con framework di deep learning diversi da TensorFlow (come PyTorch).\n",
    "  - Stai eseguendo attività di **computazione grafica** o giochi, oltre al deep learning.\n",
    "\n",
    "- **Usa una TPU** se:\n",
    "  - Stai addestrando modelli di **deep learning** su larga scala, specialmente con **TensorFlow**.\n",
    "  - Hai bisogno di un hardware altamente specializzato e ottimizzato per il deep learning.\n",
    "  - Vuoi sfruttare la **scalabilità** del cloud e migliorare l'efficienza energetica.\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Sia le **GPU** che le **TPU** sono acceleratori hardware potenti, ma sono progettate con obiettivi diversi:\n",
    "\n",
    "- Le **GPU** sono molto versatili e ottime per una varietà di calcoli paralleli, comprese le applicazioni di deep learning.\n",
    "- Le **TPU**, invece, sono specializzate nel deep learning e offrono prestazioni eccezionali per i calcoli necessari in questo campo, in particolare se usi **TensorFlow**.\n",
    "\n",
    "In sintesi, se desideri una soluzione ad alte prestazioni per il deep learning, **le TPU** potrebbero essere la scelta migliore (se usi TensorFlow), ma se hai bisogno di maggiore flessibilità o usi framework diversi, **le GPU** sono probabilmente la scelta più versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103467bb-2800-4f9a-8e05-83316e30a38f",
   "metadata": {},
   "source": [
    "# ALGEBRA RETI NEURALI\n",
    "Le **reti neurali** si basano su concetti di algebra lineare per eseguire operazioni matematiche complesse e apprendere modelli dai dati. Dietro a una rete neurale, ci sono operazioni fondamentali di **algebra lineare**, come moltiplicazioni di matrici, somme vettoriali e trasformazioni lineari, che sono essenziali per calcolare le attivazioni e ottimizzare i parametri del modello durante l'addestramento.\n",
    "\n",
    "### **Algebra Lineare nelle Reti Neurali**\n",
    "\n",
    "Vediamo alcuni dei principali concetti di algebra lineare che stanno dietro al funzionamento delle reti neurali:\n",
    "\n",
    "### 1. **Vettori e Matrici**\n",
    "I **vettori** e le **matrici** sono utilizzati per rappresentare i dati, i pesi e le attivazioni in una rete neurale.\n",
    "\n",
    "- **Vettori**: Un vettore è una colonna o una riga di numeri. Viene usato per rappresentare:\n",
    "  - Un singolo **input** (ad esempio, le caratteristiche di un'immagine o un dato di una sequenza).\n",
    "  - Un singolo **output** o **attivazione** di un neurone in un layer.\n",
    "  \n",
    "- **Matrici**: Una matrice è una tabella di numeri disposti in righe e colonne. Le matrici vengono usate per:\n",
    "  - Rappresentare i **pesi** di una rete neurale (ad esempio, la matrice dei pesi che collega il layer di input con il layer nascosto).\n",
    "  - Rappresentare i **dati batch**, dove una matrice può contenere gli input di più campioni contemporaneamente.\n",
    "\n",
    "### 2. **Operazioni Matematiche Fondamentali**\n",
    "Le reti neurali eseguono una serie di operazioni matematiche che combinano questi vettori e matrici. Le principali operazioni includono:\n",
    "\n",
    "#### **Moltiplicazione di Matrici (o Matrice-Vettore)**\n",
    "La moltiplicazione di matrici è una delle operazioni più fondamentali in una rete neurale. Durante l'addestramento, i dati di input vengono moltiplicati per una matrice di **pesi** per ottenere una nuova rappresentazione dei dati (una **attivazione**).\n",
    "\n",
    "Se hai un **vettore di input** \\( \\mathbf{x} \\) e una **matrice dei pesi** \\( \\mathbf{W} \\), la moltiplicazione \\( \\mathbf{W} \\cdot \\mathbf{x} \\) ti fornirà un nuovo **vettore di attivazione**:\n",
    "\n",
    "\\[\n",
    "\\mathbf{z} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\n",
    "\\]\n",
    "\n",
    "Dove:\n",
    "- \\( \\mathbf{W} \\) è la matrice dei pesi (dimensione \\( n \\times m \\)).\n",
    "- \\( \\mathbf{x} \\) è il vettore di input (dimensione \\( m \\times 1 \\)).\n",
    "- \\( \\mathbf{b} \\) è il **bias** (un vettore che viene aggiunto al risultato per ottenere maggiore flessibilità nel modello).\n",
    "- \\( \\mathbf{z} \\) è il vettore di **attivazione** (dimensione \\( n \\times 1 \\)).\n",
    "\n",
    "#### **Funzione di Attivazione**\n",
    "Dopo aver calcolato la somma pesata \\( \\mathbf{z} \\), viene applicata una **funzione di attivazione** a ciascun elemento del vettore \\( \\mathbf{z} \\), per introdurre non linearità nel modello. Alcune funzioni di attivazione comuni includono:\n",
    "- **Sigmoid**: \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\)\n",
    "- **ReLU** (Rectified Linear Unit): \\( \\text{ReLU}(z) = \\max(0, z) \\)\n",
    "- **Tanh**: \\( \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\)\n",
    "\n",
    "La funzione di attivazione applicata ai valori di \\( \\mathbf{z} \\) dà il **vettore di attivazione** finale per un layer.\n",
    "\n",
    "#### **Somma e Bias**\n",
    "Il termine **bias** \\( \\mathbf{b} \\) è fondamentale per rendere il modello più flessibile. Senza di esso, la rete neurale sarebbe vincolata a passare per l'origine, ma con il bias, possiamo **spostare la funzione di attivazione** e rappresentare in modo migliore le relazioni nei dati.\n",
    "\n",
    "\\[\n",
    "\\mathbf{z} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\n",
    "\\]\n",
    "\n",
    "Il bias consente di **\"spostare\"** la funzione di attivazione, permettendo al modello di adattarsi meglio ai dati.\n",
    "\n",
    "### 3. **Propagazione in avanti (Forward Propagation)**\n",
    "La **propagazione in avanti** è il processo di calcolo dell'output di una rete neurale, dove ogni layer applica una moltiplicazione matrice-vettore seguita da una funzione di attivazione.\n",
    "\n",
    "Per esempio, in una rete neurale con un singolo layer, il processo di propagazione in avanti avviene come segue:\n",
    "\n",
    "1. Moltiplichiamo l'input \\( \\mathbf{x} \\) per la matrice dei pesi \\( \\mathbf{W} \\) e aggiungiamo il bias \\( \\mathbf{b} \\).\n",
    "2. Applichiamo la funzione di attivazione per ottenere l'output del layer.\n",
    "\n",
    "Se il layer è seguito da un altro, l'output di questo layer diventa l'input per il layer successivo, e il processo continua fino all'output finale.\n",
    "\n",
    "### 4. **Backpropagation e Ottimizzazione**\n",
    "Durante il processo di **backpropagation**, la rete neurale **calcola l'errore** nell'output e propaga questo errore all'indietro attraverso i vari layer per aggiornare i pesi.\n",
    "\n",
    "L'errore tra il valore previsto e quello reale viene calcolato usando una **funzione di perdita** (ad esempio, l'errore quadratico medio o cross-entropy), e successivamente vengono calcolati i **gradienti** dei pesi usando il **gradiente discendente**.\n",
    "\n",
    "Il gradiente discendente utilizza l'**algebra differenziale** per calcolare come i pesi devono essere aggiornati per ridurre l'errore. Questo processo coinvolge la **derivata** della funzione di attivazione e la **matrice Jacobiana** per calcolare le modifiche ai pesi.\n",
    "\n",
    "### **Esempio di Backpropagation (gradiente discendente)**\n",
    "\n",
    "Nel contesto della retropropagazione, il gradiente di una funzione di perdita \\( L \\) rispetto a un peso \\( W \\) è calcolato utilizzando la **regola della catena**:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "\\]\n",
    "\n",
    "Dove:\n",
    "- \\( \\frac{\\partial L}{\\partial z} \\) è il gradiente della funzione di perdita rispetto all'attivazione.\n",
    "- \\( \\frac{\\partial z}{\\partial W} \\) è la derivata dell'attivazione rispetto ai pesi.\n",
    "\n",
    "Il gradiente discendente aggiorna i pesi come segue:\n",
    "\n",
    "\\[\n",
    "W := W - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "\\]\n",
    "\n",
    "Dove \\( \\eta \\) è il tasso di apprendimento.\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "In sintesi, le **reti neurali** si basano pesantemente sull'algebra lineare per eseguire calcoli efficienti e ottimizzare i pesi attraverso il gradiente discendente. Le operazioni principali come **moltiplicazioni di matrici**, **somma di vettori**, e **derivate** sono essenziali per l'addestramento e la predizione. L'algebra lineare consente di rappresentare e manipolare i dati in modo efficiente, facilitando l'apprendimento delle rappresentazioni complesse nei modelli di deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5826acb-dea3-4d0a-81c3-f36e820c1212",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION\n",
    "\n",
    "La **cross-validation** è una tecnica utilizzata per valutare la performance di un modello di machine learning in modo più robusto, riducendo il rischio di overfitting e ottenendo una stima più affidabile delle sue capacità su dati non visti. Viene applicata principalmente per scegliere il miglior modello o per ottimizzare i suoi iperparametri.\n",
    "\n",
    "### **Che cos'è la Cross-Validation?**\n",
    "La cross-validation consiste nel suddividere il dataset in **parti** (spesso chiamate **folds**), e nell'allenare il modello su alcuni di questi fold e testarlo su altri. Questo processo viene ripetuto più volte, garantendo che ogni punto dei dati venga utilizzato sia per l'addestramento che per il test, senza che vi sia sovrapposizione tra i dati di addestramento e quelli di test in ciascuna iterazione.\n",
    "\n",
    "La tecnica più comune è la **k-fold cross-validation**, ma esistono anche varianti, come la **leave-one-out cross-validation**.\n",
    "\n",
    "### **Passi del Processo di K-Fold Cross-Validation**\n",
    "Ecco i passi generali per applicare la **k-fold cross-validation**:\n",
    "\n",
    "1. **Dividere il dataset** in \\( k \\) parti (folds) di dimensioni più o meno uguali. Di solito, \\( k \\) è un numero come 5 o 10, ma può variare a seconda delle necessità.\n",
    "\n",
    "2. **Ripetere il processo k volte** (una per ogni fold):\n",
    "   - Per ogni iterazione, uno dei **folds** viene utilizzato come **set di test**, mentre gli altri \\( k-1 \\) folds vengono utilizzati come **set di addestramento**.\n",
    "   - **Allena** il modello utilizzando il set di addestramento e **valuta** la sua performance sul set di test.\n",
    "   - Calcola una **metrica di performance** (ad esempio, **accuratezza**, **errore quadratico medio**, **AUC**, ecc.) per ogni iterazione.\n",
    "\n",
    "3. **Calcolare la performance media**: Al termine dei \\( k \\) passaggi, la performance complessiva del modello è solitamente calcolata come la **media** delle performance ottenute in ciascuna delle iterazioni.\n",
    "\n",
    "4. **Opzionalmente, ottimizzare gli iperparametri**: La cross-validation viene spesso utilizzata per **selezionare i migliori iperparametri** di un modello. Ad esempio, si può usare la **grid search** o la **random search** in combinazione con la cross-validation per trovare la configurazione di iperparametri che fornisce la migliore performance media.\n",
    "\n",
    "### **Esempio Pratico di K-Fold Cross-Validation**\n",
    "Immagina di avere un dataset con 1000 campioni e di voler eseguire una **10-fold cross-validation**:\n",
    "\n",
    "1. Suddividi il dataset in 10 fold uguali, quindi ogni fold avrà 100 campioni.\n",
    "   \n",
    "2. Inizia la prima iterazione, dove:\n",
    "   - **Fase di allenamento**: Usa 9 fold (900 campioni) per addestrare il modello.\n",
    "   - **Fase di test**: Usa il fold rimanente (100 campioni) come set di test.\n",
    "   \n",
    "3. Ripeti il processo per ogni fold, così che ogni fold venga usato una volta come set di test.\n",
    "\n",
    "4. Alla fine, calcola la **media** delle metriche di performance (ad esempio, la media delle accuratezze ottenute in ciascun fold).\n",
    "\n",
    "### **Vantaggi della Cross-Validation**\n",
    "1. **Valutazione più robusta**: La cross-validation offre una valutazione più affidabile delle performance del modello rispetto a una semplice divisione in training e test set. Questo riduce il rischio che il modello sia troppo adattato ai dati di addestramento.\n",
    "   \n",
    "2. **Utilizzo completo dei dati**: Ogni campione viene utilizzato sia per l'addestramento che per il test, aumentando l'efficacia dell'uso dei dati, specialmente se il dataset è piccolo.\n",
    "   \n",
    "3. **Riduzione dell'overfitting**: Poiché il modello è allenato e testato su diverse sotto-divisioni del dataset, è meno probabile che il modello si adatti troppo strettamente ai dati di addestramento.\n",
    "\n",
    "4. **Ottimizzazione degli iperparametri**: Con la cross-validation, è possibile confrontare facilmente le performance dei modelli con diverse configurazioni di iperparametri.\n",
    "\n",
    "### **Svantaggi della Cross-Validation**\n",
    "1. **Costo computazionale**: La cross-validation può richiedere un numero elevato di operazioni di addestramento e test, poiché il modello deve essere addestrato \\( k \\) volte (per ogni fold). Questo può essere costoso in termini di tempo e risorse, specialmente con modelli complessi e grandi dataset.\n",
    "\n",
    "2. **Non adatta a dataset molto grandi**: Per dataset molto grandi, la cross-validation potrebbe essere computazionalmente onerosa. In questi casi, si potrebbero utilizzare tecniche più semplici come una divisione **train-test split**.\n",
    "\n",
    "3. **Non sempre adatta per dati temporali**: Quando si lavora con dati temporali o sequenziali (ad esempio, nel caso di serie temporali o dati di eventi temporali), la cross-validation tradizionale potrebbe non essere appropriata. In questi casi, potrebbe essere necessario usare tecniche specifiche come la **time-series cross-validation**.\n",
    "\n",
    "### **Varianti della Cross-Validation**\n",
    "\n",
    "1. **Leave-One-Out Cross-Validation (LOO-CV)**:\n",
    "   - In questa variante, il numero di fold è pari al numero di campioni nel dataset, quindi ogni campione viene usato come **set di test** una sola volta, mentre tutti gli altri vengono usati come **set di addestramento**.\n",
    "   - È molto computazionalmente costosa, ma può essere utile quando si hanno pochi dati.\n",
    "\n",
    "2. **Stratified K-Fold Cross-Validation**:\n",
    "   - In **stratified k-fold cross-validation**, i dati sono suddivisi in fold in modo che la **distribuzione delle classi** nel set di addestramento e nel set di test sia mantenuta il più simile possibile. Questo è particolarmente utile nei problemi di classificazione con classi sbilanciate.\n",
    "\n",
    "3. **Group K-Fold Cross-Validation**:\n",
    "   - In **group k-fold cross-validation**, i dati sono suddivisi in fold in modo che campioni appartenenti allo stesso gruppo (ad esempio, una stessa persona o un’area geografica) non vengano mai separati tra il set di addestramento e quello di test. Questo è utile in situazioni in cui i dati siano raggruppati in modo naturale.\n",
    "\n",
    "4. **Monte Carlo Cross-Validation**:\n",
    "   - Invece di dividere il dataset in un numero fisso di fold, in **Monte Carlo cross-validation** vengono create diverse **divisioni casuali** dei dati in set di addestramento e test. Il processo viene ripetuto molte volte e la performance del modello viene mediata su tutte le iterazioni.\n",
    "\n",
    "### **Conclusione**\n",
    "La **cross-validation** è una tecnica fondamentale per ottenere una stima robusta e generalizzabile delle performance di un modello di machine learning. Sebbene possa essere costosa dal punto di vista computazionale, offre vantaggi significativi, soprattutto quando si tratta di evitare overfitting e ottimizzare i modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cddd5-31a7-4e2e-880d-e34519a31cf3",
   "metadata": {},
   "source": [
    "# FUNZIONI DI ATTIVAZIONE \n",
    "Le **funzioni di attivazione** sono un componente fondamentale delle reti neurali. Sono utilizzate per introdurre non linearità nel modello, permettendo alla rete di apprendere e rappresentare funzioni più complesse rispetto a una semplice somma ponderata delle input. Senza una funzione di attivazione, la rete neurale si ridurrebbe a un semplice modello lineare, che non sarebbe in grado di catturare relazioni complesse nei dati.\n",
    "\n",
    "### **Perché sono importanti?**\n",
    "Le funzioni di attivazione sono cruciali per permettere alla rete neurale di apprendere da dati complessi, come immagini, testo o sequenze temporali. Permettono alla rete di apprendere non solo relazioni lineari ma anche **relazioni non lineari**, che sono fondamentali per la maggior parte dei problemi di machine learning, come la classificazione delle immagini, il riconoscimento del parlato, e la traduzione automatica.\n",
    "\n",
    "### **Funzionamento delle Funzioni di Attivazione:**\n",
    "Ogni neurone in una rete neurale prende in input una somma ponderata dei valori di input (usando i pesi e un eventuale bias), e poi applica una funzione di attivazione per determinare il valore che passerà al prossimo livello della rete.\n",
    "\n",
    "Formalmente:\n",
    "\\[\n",
    "y = f(w \\cdot x + b)\n",
    "\\]\n",
    "Dove:\n",
    "- \\(x\\) è il vettore di input,\n",
    "- \\(w\\) è il vettore di pesi,\n",
    "- \\(b\\) è il bias,\n",
    "- \\(f\\) è la funzione di attivazione,\n",
    "- \\(y\\) è l'output.\n",
    "\n",
    "### **Tipi di Funzioni di Attivazione Comuni**\n",
    "\n",
    "1. **Funzione di attivazione Sigmoid**\n",
    "   - Formula: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "   - L'output è compreso tra 0 e 1.\n",
    "   - **Pro**: Utile per modelli di classificazione binaria (ad esempio, nei modelli di regressione logistica).\n",
    "   - **Contro**: Può soffrire di **vanishing gradient** (dove il gradiente diventa molto piccolo durante la retropropagazione, impedendo l'apprendimento).\n",
    "\n",
    "2. **Funzione di attivazione Tanh (Tangente Iperbolica)**\n",
    "   - Formula: \\( f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "   - L'output è compreso tra -1 e 1.\n",
    "   - **Pro**: Produce un output centrato su zero, il che può accelerare l'allenamento.\n",
    "   - **Contro**: Anche la Tanh soffre del problema del **vanishing gradient**.\n",
    "\n",
    "3. **Funzione di attivazione ReLU (Rectified Linear Unit)**\n",
    "   - Formula: \\( f(x) = \\max(0, x) \\)\n",
    "   - L'output è 0 per valori negativi e uguale a \\(x\\) per valori positivi.\n",
    "   - **Pro**: È computazionalmente molto semplice, aiuta a risolvere il problema del **vanishing gradient**, ed è molto efficace per l'addestramento delle reti neurali profonde.\n",
    "   - **Contro**: Può soffrire del problema del **dying ReLU** (dove alcuni neuroni smettono di attivarsi e non apprendono mai), specialmente con valori di input molto negativi.\n",
    "\n",
    "4. **Funzione di attivazione Leaky ReLU**\n",
    "   - Formula: \\( f(x) = \\max(\\alpha x, x) \\), dove \\(\\alpha\\) è un piccolo valore positivo (di solito 0.01).\n",
    "   - **Pro**: Una versione migliorata della ReLU che cerca di risolvere il problema del **dying ReLU**, consentendo un piccolo gradiente anche per valori negativi di \\(x\\).\n",
    "   - **Contro**: Anche se aiuta, non elimina completamente il problema della **dying ReLU**.\n",
    "\n",
    "5. **Funzione di attivazione Softmax**\n",
    "   - Formula: \\( f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\), dove \\(x_i\\) è il valore dell'output per ciascuna classe e \\(x_j\\) sono gli altri valori delle classi.\n",
    "   - **Pro**: Utilizzata tipicamente nell'ultimo strato di una rete neurale per problemi di **classificazione multiclasse**. Restituisce una probabilità per ciascuna classe, in modo che la somma delle probabilità per tutte le classi sia uguale a 1.\n",
    "   - **Contro**: Non è utilizzata nei layer nascosti, ma solo nell'output di una rete per la classificazione multiclasse.\n",
    "\n",
    "6. **Funzione di attivazione ELU (Exponential Linear Unit)**\n",
    "   - Formula: \n",
    "     \\[\n",
    "     f(x) = \\begin{cases}\n",
    "     x & \\text{se } x > 0 \\\\\n",
    "     \\alpha(e^x - 1) & \\text{se } x \\leq 0\n",
    "     \\end{cases}\n",
    "     \\]\n",
    "   - **Pro**: Come la ReLU, ma con un'attivazione che continua a crescere in modo esponenziale per valori negativi, migliorando la propagazione del gradiente.\n",
    "   - **Contro**: È computazionalmente più complessa rispetto alla ReLU.\n",
    "\n",
    "### **Quando usare quale funzione di attivazione?**\n",
    "- **Sigmoid e Tanh** sono stati utilizzati ampiamente in passato, ma sono stati in gran parte sostituiti da **ReLU** e varianti come **Leaky ReLU** nelle reti più profonde.\n",
    "- **ReLU** è una scelta comune per gli strati nascosti di una rete neurale a causa della sua semplicità e velocità di calcolo.\n",
    "- **Softmax** è generalmente utilizzata nell'output per i problemi di classificazione multiclasse.\n",
    "- **ELU** è utile quando si vogliono evitare problemi di \"dying neurons\" e si cerca una maggiore robustezza.\n",
    "\n",
    "### **Riassunto dei Vantaggi e Svantaggi delle Funzioni di Attivazione**\n",
    "\n",
    "| Funzione     | Intervallo Output | Vantaggi                                              | Svantaggi                                      |\n",
    "|--------------|-------------------|-------------------------------------------------------|------------------------------------------------|\n",
    "| **Sigmoid**  | (0, 1)            | Buona per la classificazione binaria                  | Vanishing gradient, lento apprendimento       |\n",
    "| **Tanh**     | (-1, 1)           | Centra l'output, utile per migliorare l'apprendimento | Vanishing gradient, lento apprendimento       |\n",
    "| **ReLU**     | (0, ∞)            | Semplice, aiuta ad evitare il vanishing gradient       | Dying ReLU, neuroni che non si attivano mai   |\n",
    "| **Leaky ReLU** | (-∞, ∞)          | Risolve parzialmente il problema del dying ReLU       | Può ancora avere alcuni neuroni non attivi    |\n",
    "| **Softmax**  | (0, 1)            | Ideale per classificazione multiclasse, probabilità    | Solo nell'output per la classificazione       |\n",
    "| **ELU**      | (-∞, ∞)           | Maggiore robustezza e propagazione del gradiente      | Computazionalmente più complessa              |\n",
    "\n",
    "Le funzioni di attivazione sono una delle chiavi per il successo delle reti neurali moderne, permettendo di risolvere problemi non lineari e di migliorare l'efficacia dell'apprendimento, specialmente in reti profonde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d54fb2-8082-47fa-90e8-48d19c27c224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
